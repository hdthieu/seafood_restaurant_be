import { Embeddings } from "@langchain/core/embeddings";
import type { AsyncCallerParams } from "@langchain/core/utils/async_caller";
import { LlmGateway } from "../ai/llm.gateway"; // tuá»³ báº¡n Ä‘á»ƒ path

export class GatewayEmbeddings extends Embeddings {
  constructor(private readonly llm: LlmGateway, params?: AsyncCallerParams) {
    super(params ?? {}); // ğŸ‘ˆ báº¯t buá»™c truyá»n object vÃ o Ä‘Ã¢y
  }

  // embed 1 cÃ¢u há»i
  async embedQuery(text: string): Promise<number[]> {
    const v = await this.llm.embed(text); // dÃ¹ng Ä‘Ãºng hÃ m embed báº¡n Ä‘Ã£ xÃ i Ä‘á»ƒ upsert vÃ o Qdrant
    // v cÃ³ thá»ƒ lÃ  number[] hoáº·c number[][]
    if (Array.isArray(v) && Array.isArray(v[0])) {
      return v[0] as number[];
    }
    return v as number[];
  }

  // embed nhiá»u document
  async embedDocuments(texts: string[]): Promise<number[][]> {
    const v = await this.llm.embed(texts); // embed batch
    if (!Array.isArray(v)) {
      throw new Error("embedDocuments return invalid");
    }
    if (Array.isArray(v[0])) {
      return v as number[][];
    }
    // lá»¡ Ä‘Ã¢u backend tráº£ vá» 1 vector cho táº¥t cáº£ -> bá»c láº¡i
    return [v as number[]];
  }
}
